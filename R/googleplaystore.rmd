Abstract
The data set we have chosen is regarding Google Play Store Apps with various aggregation types including app rating, app function category (e.g. game, business, communication, etc.), number of user reviews, size of app, etc. 
https://www.kaggle.com/lava18/google-play-store-apps

Variables
App: application name
Rating: overall user rating of the app (when scraped)
Reviews: number of user reviews for the app (when scraped)
Installs: number of user downloads for the app (when scraped)
Type: Paid or Free-To-Install (App)
Genre: genre that the application belongs to

Summary Statistics



The questions we have come up after initial investigation of the data are as follows:
Do free-to-install apps get significantly more installs than paid ones on the Google Play Store? 
Are app ratings correlated with the number of installs on the Google Play Store?
Is number of reviews correlated with the number of installs on the Google Play Store?
Are the average numbers of installs significantly different across app genres on the Google Play Store?

We believe these questions are relevant for app developers in analyzing their products to increase popularity, visibility, and profitability of their products. 

Through one-sided two-sided t-test for question 1, two-sided test for correlation for question 2 and 3, and ANOVA F-test for question 4, we found the following: Free apps are installed more than paid apps, app rating to installs correlation is weak across apps of all popularities, number of reviews positively correlated to the number of installs per app, and not all app genres are created equally and app developers should consult the most popular genres carefully and ensure that they are not modeling their genres after outliers.




End of abstract.
Introduction
It is without a doubt that software development promises great potential for success, so it is no wonder that the Google Play Store and the Apple Store are saturated with thousands of different apps that have become an essential part of our lives today. Perhaps the market is oversaturated; for example, there are hundreds of calculator apps to choose from, many of which are no different from the others. This creates one of the greatest challenges modern-day software developers have to face, and it is in the best interest of these creators to figure out strategies that would optimize their success in this highly competitive market.
	Although performance and quality are important in making the application stand out amongst others, we must investigate other factors that could potentially draw the users into downloading the product. For example, before publishing a new app, the developer must make several decisions such as whether to make it downloadable for free or at a certain price. Within the program, some developers even implement reminders that encourage users to rate or write reviews in hopes to promote their apps even more. Maybe developers should focus on a specific app genre in order to maximize the number of installs. This then raises several questions that we can investigate:

Do free-to-install apps get significantly more installs than paid ones on the Google Play Store? 

Are app rating scores correlated with the number of installs on the Google Play Store?

Is number of reviews correlated with the number of installs on the Google Play Store?

Are the average numbers of installs significantly different across app genres on the Google Play Store?


All of the statistical operations and tests were conducted through the R software, analyzing a Kaggle data frame that details a variety of information about all the apps available on the Google Play Store in 2018.

Background Information and Methods
Significant tests with this 2018 data will be used for predictive analysis for near future market trends. As this data contains apps on the Play Store since its conception, we can effectively conclude that, for all intents and purposes, this data of 10k apps is an accurate representation of all apps on the Play Store, and is sufficient for the questions we will be answering in this report.

The team first modified the original dataset via MS Excel to properly format the elements of the â€œInstallsâ€ column, as the original formatting included commas and â€œ+â€s at the end of each element. The team erased all instances of the characters mentioned above.

Do free-to-install apps get significantly more installs than paid ones on the Google Play Store?

	In order to answer this question, we will use the one-sided two-sample t test for the difference in means between two different groups of data: Free and Paid, which are options for the Type column of the dataframe. The test choice is appropriate as we want to evaluate if the population mean for installation counts in free apps is greater than that of paid apps, and the one-sided two-sample t test for the difference in means does just that.
The hypotheses are as follows:
ğ»0 : ğœ‡f = ğœ‡p
ğ»a : ğœ‡f > ğœ‡p
 ğœ‡f  = population mean installs for free apps on the Google Play Store in 2019,  ğœ‡p = population mean installs for paid apps on the Google Play Store in 2019
	The following are assumptions for question 1 and their rationales:
The samples follow a normal distribution.
Sample sizes are large enough, most definitely greater than 30, so by the central limit theorem, the samples are approximately normal.
The samples are independent between and within.
For independence between, we make the assumption that the number of installs does not impact that of the other when comparing free and paid apps, disregarding special cases such as when one type of app fills up the rest of the device storage and prevents further downloads of the other type. We claim independence within by assuming that the sum of Google Play Store apps in the cumulative catalog across time is at least ten times greater than each paid and free app sample.
The samples were selected without sampling bias.
Usually whether or not simple random sampling was used or not is the condition that is checked, but refer to the first 2 sentences of this section, Background Information and Methods.







Are app rating scores correlated with the number of installs on the Google Play Store?

For this question, we will conduct a two-sided test for correlation (Pearson Correlation) in order to visualize the correlation between app ratings and number of installs on the Google Play Store. App Ratings is considered as a continuous variable and Number of Installs is considered as a discrete variable, but since we converted Number of Installs to a numerical, we are able to compare the two and see how they correlate. A two-sided test for correlation is a good way to test this as it tests if app ratings and number of installs are correlated or not (correlation coefficient = 0). 

The hypotheses:
ğ»0 : r = 0
ğ»a : r â‰  0
r = the correlation coefficient that measures the strength of a linear relationship between population app ratings and population number of installs.


The following are the assumptions of question 2:
Level of measurements
Both variables are continuous, so a Pearson correlation works here (number of Installs was modified to fit this requirement, as discussed above).
Linearity
Data seems to have a â€œstraight-lineâ€ relationship, so the condition is met.
Related pairs
Each app in the data has an â€œinstall numberâ€ and rating (1 sample that does not meet this requirement was removed, discussed in Results).
Absence of outliers
No visible outliers after filtering the data, so the condition is met.

Is number of reviews correlated with the number of installs on the Google Play Store?


This question is very similar to question 2 with a different variable. We will also conduct a two-sided test for correlation between the number of Installs and the number of Reviews. Number of Reviews is considered as a continuous variable and number of Installs is considered as a discrete variable, but since we converted Number of Installs to a numerical, we are able to compare the two and see how they correlate. A two-sided test for correlation is a good way to test this as it tests if the number of reviews of apps and number of installs of apps are correlated (correlation coefficient = 0). 

The hypotheses:
ğ»0 : r = 0
ğ»a : r â‰  0
r = the correlation coefficient that measures the strength of a linear relationship between population number of reviews and population number of installs.

The following are the assumptions of question 3:
Level of measurements
Both variables are continuous, so a Pearson correlation works here (number of Installs was modified to fit this requirement, as discussed above).
Linearity
Data seems to have a â€œstraight-lineâ€ relationship, so the condition is met.
Related pairs
Each app in the data has an â€œinstall numberâ€ and number of reviews (1 sample that does not meet this requirement was removed, discussed in Results).
Absence of outliers
No visible outliers after filtering the data, so the condition is met.

Note: The team was aware that, because the popularity of apps (i.e. number of installs for all apps) followed the Pareto principle, most apps received 0 to insignificant number of installs while a select few number of apps occupied most of the market (refer to figure below). This means that the sample size for the number of reviews of a rarely-installed app would potentially be too small and may potentially disrupt our tests for questions 2 and 3, since every app regardless of popularity has equal weight in our model. Taking this issue into consideration, the team decided to apply questions 2 and 3 into three samples:

1. The original dataset with all numbers of installs (n = 10840)
2. Dataset that only contained elements with 1,000+ installs (n = 9037)
3. Dataset that only contained elements with 1,000,000+ installs (n = 2081)

The team determined these cutoff points for the three respective samples for the following reasons:
	
To test the original data without any manipulation as control
To exclude all data that the team considered too unpopular (too few installs) to be significant
To test only highly popular apps.







Are the average numbers of installs significantly different across app genres?

In order to answer this question, we will use ANOVA comparing the average number of installations across different app genres present in the data set. Like in previous questions, Installs was converted to a numeric type beforehand. An ANOVA F-test was deemed appropriate by the team as we wanted to see if there were any statistically significant differences in the average number of app installs between the individual, independent app genres. This test would allow us to conclude that app genre does matter when it comes to gaining the highest chance of success in the market.
The hypotheses are as follows:
Null Hypothesis: 1 = 2 = â‹¯ = i
Alternative Hypothesis: At least one i differs
i = population mean number of installations of all unique app genres.

	The following are assumptions for question 4 and their rationales:
Randomness and independence of samples
Usually whether or not simple random sampling was used or not is the condition that is checked, but refer to the first 2 sentences of this section, Background Information and Methods.
Equal variances
Condition is met since all groups have roughly equal variance.
Normality
We tested the normality of the average number of installations per genre by dividing the total number of installs per genre by the total count of apps per genre. The resulting values are plugged into a qq plot:

> meanInstalls = aggregate(mydata10841[,6], by = list(mydata10841$Genres), FUN = mean)
> x = as.data.frame(table(mydata10841$Genres))
> y = meanInstalls[order(meanInstalls$Group.1, decreasing = FALSE),]
> x[,"mean"] = y[,2] / x[,2]
> x[,"sum"] = y[,2]
> qqplot(x$mean, x$sum)


The team is aware the qq plot does not look like a traditional linear line due to the number of installs per genre following the Pareto distribution and thus having fewer samples on the upper right quadrant. Thus, the abundance in outliers may provide inaccuracies. However, the team considers this plot still roughly linear and thus passed the normality test, keeping in mind the likelihood of inaccuracies in results.
	







Results
The R code for each research question is shown below.


The team ran this code before performing the tests. This deletes an element with bad data; values in the Installs column of the dataframe should be numeric data, but instead is â€œFreeâ€ and makes no sense. Values in the Reviews column is numeric, but instead is â€œ3.0Mâ€ which does not follow the format of others, while converting the entry to 3000000 leaves it at a disparate outlier and thus raises suspicion to its authenticity. Such inconsistencies in the data convinced the team to just remove this element.
Note, also, that the team used â€œdsâ€, â€œgpâ€, and â€œmydataâ€ interchangeably as variables all referring to the original dataset.

Do free-to-install apps get significantly more installs than paid ones on the Google Play Store? 	








Null Hypothesis: There is no significant difference in number of installs between paid and unpaid apps.
Alternative Hypothesis: Number of installs of unpaid apps is significantly greater than that of paid apps.
Conclusion: p-value = 2e-16 < alpha = 0.05 â†’ reject the null hypothesis. We have sufficient evidence to conclude that free-to-install apps do get more installs than paid ones on the Google Play Store.


Are app rating scores correlated with downloads? (two-sided test for correlation between paired samples)
Null Hypothesis: There is no correlation between the Ratings and number of Installs.
Alternative Hypothesis: There is a correlation between the Ratings and the number of Installs.



Conclusion: p-value = 2e-16 < alpha = 0.05 â†’ reject the null hypothesis. Since the correlation coefficient is 0.05, there is a weak correlation between number of Installs and Ratings when considering all apps on the Google Play store. We think this is due to the large number of apps with few or without any installs.

(Apps with 1000 or more Installs)

Conclusion: p-value = 5.74e-09 < alpha = 0.05 â†’ reject the null hypothesis. 
(Apps with 1,000,000 or more Installs)	
Conclusion: p-value = 0.002665 < alpha = 0.05 â†’ reject the null hypothesis. 
Final Conclusion: The correlation between App Ratings and Installs is about 0.05 regardless of app popularity.


Are numbers of reviews correlated with downloads? (two-sided test for correlation between paired samples)
Null Hypothesis: There is no evidence for significant correlation between number of installs and ratings.
Alternative Hypothesis: There is evidence for significant correlation between number of installs and ratings.

The team performed this test for the dataset with all elements:


Conclusion: p < 0.05, reject H0. Since the value is 0.64, there is a moderate correlation between number of installs and the number of reviews.


The team performed this test for the dataset with elements with 1,000+ installs:
> cor.test(mydata1000$Installs, mydata1000$Reviews, alternative = "two.sided")

Conclusion: p < 0.05, reject H0. Since the value is 0.64, there is a moderate correlation between number of installs and the number of reviews.

The team performed this test for the dataset with 1,000,000+ installs:
> cor.test(mydata1M$Installs, mydata1M$Reviews, alternative = "two.sided")

Conclusion: p < 0.05, reject H0. Since the value is 0.60, there is a moderate correlation between number of installs and the number of reviews.

Final conclusion: The correlation value of number of installs and number of reviews regardless of app popularity is about 0.6.











Are the average numbers of installs significantly different across app genres?

Null Hypothesis: ğœ‡1 = ğœ‡2 = â‹¯ = ğœ‡i (ğœ‡i = mean installs for different app genres)
Alternative Hypothesis: At least one ğœ‡ğ‘– differs
ğœ‡i = population mean number of installations of app genre i.

Conclusion: p < 0.05, reject the null hypothesis. We have statistically significant evidence that the average number of installs significantly differ across app genres.
Taking into account the large number of outliers in this experiment, this conclusion should be accepted with trepidation.

If we wish to see the genres with the highest average installs, a significant portion of them are outliers:


Only in the lower sections do we begin seeing more comfortable numbers of apps in each genre. 






Overall Conclusion
From the factors that we analyzed and the results accrued from the Google Play Store dataset, we can conclude that free-to-install apps generate more installs than apps that require a paywall to install. However, this does not necessarily imply that free-to-install apps generate more revenue, as there can be microtransactions in each app.
From the findings in the dataset, app rating to app installs correlation is weak across apps of all popularities. We may presume that quality and user-experience for an app does not seem to be related to app popularity, or that app ratings may not be accurate reflections of user satisfaction.
Number of reviews positively correlate to the number of installs per app, regardless of the popularity of the app. Note that these are not evidence of causation. The team cannot advise whether marketing for higher number of reviews would result in more installs or if high number of ratings of any level is just an effect of more installs, or if both statements are true.
Genres are not all created equally, as they follow a Pareto distribution in average installs of apps in each genre. An app developer should consult the most popular genres carefully and ensure that they are not modeling their genres after outliers. 
Thus, from the data the team has gathered, we advise that app developers push for free-to-install apps, encourage user reviews regardless of rating scores, and keep in mind certain app genres are more successful in terms of getting installations.


References
Gupta, Lavanya. (2018; December). Google Play Store Apps, Version 6. Retrieved November 
18, 2019 from https://www.kaggle.com/lava18/google-play-store-apps

